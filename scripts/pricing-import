#! /usr/bin/env bash

set -eu -o pipefail

readonly db_name=${DB_NAME:-db_dev}
readonly db_user=${DB_USER:-postgres}
readonly db_host=${DB_HOST:-localhost}

readonly path="tmp/db/pricing_import_dump.sql"

#
# Ensure Pricing Parser has been run
#

cat << EOM
To get started, make sure you have run the pricing parser on your most recent ghc pricing template
Overview of steps is in the wiki at https://github.com/transcom/mymove/wiki/ghc-pricing-import
Setup:
  make db_dev_reset db_dev_migrate
  rm -f bin/ghc-pricing-parser && make bin/ghc-pricing-parser
  ghc-pricing-parser --filename <path_to_xlsx_pricing_template> --display --contract-code=<Unique Code>

Due to the size of pricing template, this may take several minutes to complete
EOM

read -r -p "Have you run the ghc pricing parser? (y/n) " answer

while true
do
  case $answer in
   [yY]* ) pg_dump -h "$db_host" -U "$db_user" "$db_name" -t re_* --data-only -T re_services* --data-only > "$path"
    echo "Success pg-dump to tmp/db/pricing_import_dump.sql"
    break;;
  [nN]* ) echo "See above instructions to run ghc pricing importer"
    exit;;
  * )     echo "enter Y or N, please."; exit;;
  esac
done

#/bin/bash ./scripts/generate-secure-migration "import_pricing_data"

# directions on https://github.com/transcom/mymove/wiki/migrate-the-database#secure-migrations

readonly dir="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

readonly local_secure_migrations_dir="${dir}/../migrations/app/secure"
readonly prod_migrations_temp_dir="${dir}/../tmp"

#
# Pre-flight checks
#

echo "Open tmp/db/pricing_import_dump.sql and copy the contract name in the re_contracts table, paste it here followed by [ENTER]:"

read -r unique_contract_name

if [[ -z ${unique_contract_name:-} ]]; then
  echo "usage: $0 <unique_contract_name>"
  exit 1
fi

if [[ ! -d "$prod_migrations_temp_dir" ]]; then
  mkdir "$prod_migrations_temp_dir"
fi

readonly migration_name="import_pricing_data_${unique_contract_name}"

#
# Build secure migration file names
#

# Generate the secure migration filename:
# - remove ".up.fizz" from filename
# - remove "migrations/" from filename
# - append ".sql"
readonly version=$(date +"%Y%m%d%H%M%S")
readonly secure_migration_name="${version}_${migration_name}.up.sql"

readonly local_test_migration_name="${local_secure_migrations_dir}/${secure_migration_name}"
readonly prod_migration_name="${prod_migrations_temp_dir}/${secure_migration_name}"

#
# Build secure migration stub content
#

# Create empty file for local test migration
cat > "${local_test_migration_name}" << EOM
-- Local test migration.
-- This will be run on development environments.
-- It should mirror what you intend to apply on prod/staging/experimental
-- DO NOT include any sensitive data.
EOM

# Create empty file for real secure migration
cat > "${prod_migration_name}" << EOM
-- Production Migration!
-- This will be distributed to all deployed environments (e.g. experimental, staging, prod)
EOM

#
# Display next steps for user
#

cat << EOM
Generated migration files:

PG Dump: ${path}
Local test migration: ${local_test_migration_name}
Production migration: ${prod_migration_name}

Next:
    1. Open the pg dump file for changes to be deployed
    2. Edit the production migration to have the actual change you wish to deploy
    3. Copy the production migration into the local secure migration, scrubbing sensitive data
    4. If everything looks good, upload the migration to S3 with this utility:
       scripts/upload-secure-migration \\
          ${prod_migration_name}
    5. Run make run_prod_migrations to verify that the upload worked and that the migration can be applied successfully.
    6. Open a pull request for this change; when it is accepted, your migration will run on staging.
    7. Delete local test and production migrations
EOM

#
# Update the migrations manifest
#

./scripts/update-migrations-manifest
